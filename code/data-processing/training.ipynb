{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "## Models to train:\n",
    "- Classifiers:\n",
    "  - Identify if observed orientation changes caused by head or phone, spit out class (movement) + cause (head vs phone)\n",
    "  - Identify gesture, spit-out class (gesture performed, which will account for the cause also, e.g. head-turned-left vs phone-moved-right) - **Probs not this one**\n",
    "- Regression:\n",
    "  - Provide a actual value for the relative orientation + position of head and phone (may also need a classifier for the gesture)\n",
    "\n",
    "## Variations:\n",
    "- Single Model:</br>\n",
    "  Take image, output gesture</br>\n",
    "  Can only be called when image available and prior prediction complete, as such need to average other data (IMU)</br>\n",
    "  Might not be possible to have *cond* tensor (e.g. change tensor used based on some condition), so may need a multiply based on input, e.g. provide 0 if face not found, and previous image given?\n",
    "- Split Model:</br>\n",
    "  Take image, get head pose, feed into second model with motion data to get gesture</br>\n",
    "  Second model called as and when data (IMU or 1st model output) available (keeping prior values for pending data), first model called as and when image available</br>\n",
    "  Can also be called with data averaged (e.g. only when image available)</br>\n",
    "  Possibly use conditional tensor, will use different tensor object based on condition (e.g. face is visible in frame)\n",
    "- Half Model:</br>\n",
    "  Use model but only for some processing, e.g. just get landmarks (see: agarwal2020realtime & guobing2021headpose), or to get gesture\n",
    "\n",
    "Need to worry about whether tools used require region containing face to do their work, and if so how best to obtain region containing face that doesn't care for orientation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "Preparing the data to use (e.g. read the csv into pandas dataframe)\n",
    "\n",
    "Need to pre-load the images? Rotate them and resize them?</br>\n",
    "Probably too large to fit all into memory (can exclude some for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import csv\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import itertools as iter\n",
    "\n",
    "CLASSES = {\n",
    "    'NO_GESTURE' : 0,\n",
    "    'POINTING_TRANSLATE_PHONE_TOP_CENTRE' : 1,\n",
    "    'POINTING_TRANSLATE_PHONE_TOP_RIGHT' : 2,\n",
    "    'POINTING_TRANSLATE_PHONE_MID_RIGHT' : 3,\n",
    "    'POINTING_TRANSLATE_PHONE_BOTTOM_RIGHT' : 4,\n",
    "    'POINTING_TRANSLATE_PHONE_BOTTOM_CENTRE' : 5,\n",
    "    'POINTING_TRANSLATE_PHONE_BOTTOM_LEFT' : 6,\n",
    "    'POINTING_TRANSLATE_PHONE_MID_LEFT' : 7,\n",
    "    'POINTING_TRANSLATE_PHONE_TOP_LEFT' : 8,\n",
    "    'POINTING_ROTATE_PHONE_TOP_CENTRE' : 9,\n",
    "    'POINTING_ROTATE_PHONE_TOP_RIGHT' : 10,\n",
    "    'POINTING_ROTATE_PHONE_MID_RIGHT' : 11,\n",
    "    'POINTING_ROTATE_PHONE_BOTTOM_RIGHT' : 12,\n",
    "    'POINTING_ROTATE_PHONE_BOTTOM_CENTRE' : 13,\n",
    "    'POINTING_ROTATE_PHONE_BOTTOM_LEFT' : 14,\n",
    "    'POINTING_ROTATE_PHONE_MID_LEFT' : 15,\n",
    "    'POINTING_ROTATE_PHONE_TOP_LEFT' : 16,\n",
    "    'POINTING_ROTATE_HEAD_TOP_CENTRE' : 17,\n",
    "    'POINTING_ROTATE_HEAD_TOP_RIGHT' : 18,\n",
    "    'POINTING_ROTATE_HEAD_MID_RIGHT' : 19,\n",
    "    'POINTING_ROTATE_HEAD_BOTTOM_RIGHT' : 20,\n",
    "    'POINTING_ROTATE_HEAD_BOTTOM_CENTRE' : 21,\n",
    "    'POINTING_ROTATE_HEAD_BOTTOM_LEFT' : 22,\n",
    "    'POINTING_ROTATE_HEAD_MID_LEFT' : 23,\n",
    "    'POINTING_ROTATE_HEAD_TOP_LEFT' : 24,\n",
    "    'TRANSLATE_PHONE_TOP_CENTRE': 25,\n",
    "    'TRANSLATE_PHONE_MID_RIGHT': 26,\n",
    "    'TRANSLATE_PHONE_BOTTOM_CENTRE': 27,\n",
    "    'TRANSLATE_PHONE_MID_LEFT': 28,\n",
    "    'CIRCULAR_PHONE_CLOCKWISE': 29,\n",
    "    'CIRCULAR_PHONE_ANTI_CLOCKWISE': 30,\n",
    "    'CIRCULAR_HEAD_CLOCKWISE' : 31,\n",
    "    'CIRCULAR_HEAD_ANTI_CLOCKWISE' : 32,\n",
    "    'ZOOM_PHONE_ZOOM_IN' : 33,\n",
    "    'ZOOM_PHONE_ZOOM_OUT' : 34,\n",
    "    'ZOOM_HEAD_ZOOM_IN' : 35,\n",
    "    'ZOOM_HEAD_ZOOM_OUT' : 36,\n",
    "    'ROTATE_HEAD_TOP_CENTRE' : 37,\n",
    "    'ROTATE_HEAD_MID_RIGHT' : 38,\n",
    "    'ROTATE_HEAD_BOTTOM_CENTRE' : 39,\n",
    "    'ROTATE_HEAD_MID_LEFT' : 40,\n",
    "    'ROTATE_PHONE_ROLL_CLOCKWISE' : 41,\n",
    "    'ROTATE_PHONE_ROLL_ANTI_CLOCKWISE : 42'\n",
    "    'ROTATE_HEAD_ROLL_CLOCKWISE' : 43,\n",
    "    'ROTATE_HEAD_ROLL_ANTI_CLOCKWISE' : 44\n",
    "}\n",
    "\n",
    "OUTPUT_SHAPES = ((1), 1)\n",
    "OUTPUT_TYPES = (tf.dtypes.float32, tf.dtypes.uint8)\n",
    "\n",
    "CSV_FIELD_NAMES = [\n",
    "    'MS_DELTA', 'RELATIVE_IMAGE_PATH', 'LINEAR_X', 'LINEAR_Y', 'LINEAR_Z', 'ROTATION_X', 'ROTATION_Y', 'ROTATION_Z', 'ROTATION_SCALAR', \n",
    "    'LINEAR_X_DELTA', 'LINEAR_Y_DELTA', 'LINEAR_Z_DELTA', 'ROTATION_X_DELTA', 'ROTATION_Y_DELTA', 'ROTATION_Z_DELTA', 'ROTATION_SCALAR_DELTA'\n",
    "]\n",
    "# CSV_CLASSIFIER_FIELD_NAMES = []\n",
    "CSV_REGRESSION_FIELD_NAMES = [\n",
    "    'HEAD_X', 'HEAD_Y', 'HEAD_Z', 'HEAD_PITCH', 'HEAD_ROLL', 'HEAD_YAW', 'PHONE_X', 'PHONE_Y', 'PHONE_Z', 'PHONE_PITCH', 'PHONE_ROLL', 'PHONE_YAW', \n",
    "    'HEAD_X_DELTA', 'HEAD_Y_DELTA', 'HEAD_Z_DELTA', 'HEAD_PITCH_DELTA', 'HEAD_ROLL_DELTA', 'HEAD_YAW_DELTA', \n",
    "    'PHONE_X_DELTA', 'PHONE_Y_DELTA', 'PHONE_Z_DELTA', 'PHONE_PITCH_DELTA', 'PHONE_ROLL_DELTA', 'PHONE_YAW_DELTA'\n",
    "]\n",
    "\n",
    "def get_preprocessed_image(image_path):\n",
    "    cv.imread()\n",
    "    pass\n",
    "\n",
    "# Need to batch (chunk size is hyper-param)\n",
    "# probs want to over-crank to get cases where have less frames (e.g. keep every x frames, average data between that)\n",
    "def data_generator(paths, chunk_size=10, pick_rate=[1,2,3,4], regression_output=False):\n",
    "    config = \n",
    "    if average_until_new_image:\n",
    "        data_paths = shuffle(data_paths + [(path, True) for path in data_paths])\n",
    "    \n",
    "    for path in data_paths:\n",
    "        with open(path, 'r') as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "            reader.__next__() # skip header\n",
    "            for row in reader:\n",
    "                \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = # Replace with path\n",
    "def get_data_filepaths(root_dir):\n",
    "    for participant_path in root_dir.iterdir():\n",
    "        if participant_path.is_dir():\n",
    "            synced_data_path = participant_path / 'synced_data'\n",
    "            for participant_data_path in synced_data_path.iterdir():\n",
    "                if participant_data_path.is_dir():\n",
    "                    for gesture in participant_data_path.iterdir():\n",
    "                        for direction in gesture.iterdir():\n",
    "                            if len(list((direction / 'images').iterdir())) > 0:\n",
    "                                csv = synced_data_path / f'{participant_data_path.name}_{gesture.name}_{direction.name}.csv'\n",
    "                                if Path(csv).is_file():\n",
    "                                    yield csv\n",
    "\n",
    "file_paths = list(get_data_filepaths(DATA_PATH))\n",
    "print(f'Number of files to process: {len(file_paths)}')\n",
    "\n",
    "train_val_paths, test_paths = train_test_split(file_paths, test_size=0.2, random_state=28)\n",
    "train_paths, val_paths = train_test_split(train_val_paths, test_size=0.3, random_state=16)\n",
    "\n",
    "print(f\"Testing paths: {len(test_paths)}, Training: {len(train_paths)}, Validation: {len(val_paths)}\")\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(data_generator(train_paths))\n",
    "val_ds = tf.data.Dataset.from_generator(data_generator(val_paths))\n",
    "test_ds = tf.data.Dataset.from_generator(data_generator(test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('diss')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "669cd1d434545bf58bc3edc245c962a9a1da7f94287adf7d982064b9c729a651"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
