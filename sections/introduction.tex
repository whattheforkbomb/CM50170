\section{Introduction}\label{sec:intro} % 0.5 Pages
%% What 
% State of The World:
Touchscreens have been the primary interface with which users interact with modern smartphones, either through directly touching UI elements (such as buttons or an on-screen keyboard) or through the use of touch-gestures. 
Over recent years there has been a trend of smartphone touchscreens increasing in size~\cite{xuesheng2018research}, which does not afford optimal reachability of the full screen for the thumb for interactions~\cite{le2018fingers}. This reduces usability for one-handed interaction, which is a common mode of use~\cite{hoober2013users}.
\\\\
An emerging solution to this is to include head gestures as an additional mode of input, by tracking the head via the smartphone's front-facing camera~\cite{gorodnichy2004nouse, deepateep2020facial, voelker2020headreach, roig2015face, hansen2006use, francone2011using}.
A problem with tracking something via a camera that can have a moving Point-of-View (PoV) is that changing the camera's PoV can \textit{look} like movement of the object being tracked.
For example, the user moving their phone to the left, will be treated the same as the user moving their head to the right, since the positioning and movement of the head from the front-facing camera's point of view will look the same, see \autoref{fig:teaser}.
Some papers knew of this issue and accept it as a feature~\cite{hansen2006use}, others note it as a known fault to be aware of~\cite{francone2011using, varona2008hands}, while others don't indicate whether this has been accounted for~\cite{gorodnichy2004nouse, deepateep2020facial, voelker2020headreach,roig2015face}.
\\\\
% Research Question:
In this paper we look to propose a system that can distinguish between the head or smartphone being moved. In being able to differentiate the two, it should also be able to recognise gestures based on the smartphone movement.

% Solution/What We Did:
In order to develop a proof-of-concept, a data driven approach was taken.
As such a study was performed to collect data: image sequences from the front-facing camera of the smartphone, IMU data from the smartphone, and 3D positioning of the user's head and the smart device via a motion capture system (to provide a ground truth).

With the motion capture data being synced to the IMU data and images, a system could be trained to recognise several gestures and learn to distinguish between whether an observed gesture was due to the smartphone or the user's head moving.

\tempnote{Pending confirmation of system's performance (what was actually delivered/produced) and how it could be extended upon.}
% What was the system's performance? Did we succeed?
% How is the work to be extended?
