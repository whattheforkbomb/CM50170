% TODO: Do we need summaries after each subsection?
\section{Literature Review} % 2-3 Pages
\tempnote{Do I need to explain the tech, or just the name is enough, e.g. Haar cascades, CNNs, etc...}
In this section we will review existing literature to build an understanding of: the gestures we can expect to process, how they may be used and what they mean; Methods with which to obtain data pertaining to the pose of a head; and finally the means with which we can track movement and determine the gesture being performed.
% Types of Head Gestures (Pointing (Dietic & Manipulative) vs Semaphoric)
%   Since used by the systems mentioned in the intro, discuss these 3 types
%   Why would these types of gestures be used, what roles did they serve.
%   Examples of these gestures (with regards to systems referenced in intro)
% Probably a small section

% Head Tracking (From Smartphones)
% How track head, (Front facing camera, )
% Cascades, need face first, or one model (if using NN/ML solutions)
%   - IMU (From Head Mounted Displays and Head Mounted IMU)
%       - Will intend to use earbuds to collect IMU data of head, make footnote mentioning that this this was unfortunately not completed within this paper
%   - CNNs
%   - HAAR
%   - Alternatives (Feature extraction via histograms, etc)

% Head Gesture Recognition
% How did reviewed systems do this?
% Given data, how determine gesture performed, e.g. Dietic pointing (use raw data, e.g. calculate screen region/pointer position from data) vs Semaphoric/Manipulative classifying (given data, determine gesture performed)
%   - Derived
%   - Regression NN?
%   - RNN/LSTM (papers not specific to mobile phones)
%       - given a sequence, what class does it belong to?
%   - Markov Chain
%       - Given current state and new input, what is the new state?

% Merge last 2 sections into one?

\subsection{Gesture Classifications and Usage} % Types of Head Gesture
% Types of Head Gestures (Pointing (Dietic & Manipulative) vs Semaphoric)
%   Since used by the systems mentioned in the intro, discuss these 3 types
%   Why would these types of gestures be used, what roles did they serve.
%   Examples of these gestures (with regards to systems referenced in intro)
% Probably a small section

% Gesture classes and then discuss their examples / use-cases?
% Or
% Papers and examples, and then classification?

% Discuss why might want head gestures ?
%   - Any cases where these motions may be useful for SIIDs (Situationally Induced Impairments or Disabilities)?
% add more input options, not rely on using intuitive gestures that have since become system gestures (android navigation gestures) \cite{hueber2020headbang}

% Outline the types of gesture, and of those, which ones head gestures can b classified as.
% From example papers regarding head gesture control for mobile devices, look into 

Given our goal is to develop a means to distinguish head and phone gestures on smartphone devices, we first need to understand the gesture's we want to recognise and distinguish.
Here we will look at existing literature that outline the head and phone gestures you would expect to use while interfacing with a smartphone.
\\\\
% Mention classes defined > mention ignore 2 as unrelated to head-gestures > explain exemplars of each type > Summary suggesting multiple employed by each system?
% Mention classes defined and list > mention ignore 2 as unrelated to head-gestures > describe several exemplars and what style/s they fit into?
% Mention classes defined and list > mention ignore 2 as unrelated to head-gestures > describe how in our review of literature that we classify as pointing/cursor manipulation vs semaphoric (gesture maps to action)
%
% https://eprints.soton.ac.uk/261149/1/GestureTaxonomyJuly21.pdf
% Breakdown the relevant gestures
After a review of gestures utilised within Human Computer Interaction literature \citeauthor{karam2005taxonomy} define five distinct classes with which we can differentiate between types of gestures utilised by the systems proposed in the literature\cite{karam2005taxonomy}:
\begin{description}
    \item[Deictic] Gestures that involve pointing, and mapping this to either a specific object or location within the interface.
    \item[Manipulative] A gesture which indicates intent to manipulate some object.
    \item[Semaphoric] Gestures which map to a specific action or intent.
    \item[Gesticulative] Typically accompany speech, but not required to transfer meaning.
    \item[Language] A substitute to written or verbal language.
\end{description}

In our review of head/phone gesture systems we found that none utilised the Language or Gesticulative gesture styles, which is to be expected as we were focusing on gestures for control and interaction rather than for communication.
Of the 3 remaining gesture styles, we noted that systems rarely utilised a single gesture style. Either due to the gestures themselves being viewable as multiple gesture styles, being both semaphoric and manipulative, or by actively including different styles of gestures, such as pointing to a region, then using a sempahoric gesture to trigger an action.
% detail examples showing the gestures they decided upon, and why.
% example the types of gestures
% how might a phone gesture differ from head
% why might they look the same?
% yan2018headgesture, they explored existing head gestures (maybe focusing on those trackable via IMU over other methods?). Their study resulted in details shown in figure
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/yan2018headgesture_fig2_proposed_gestures.png}
    \caption{\label{fig:yan2018headgesture_proposed_gestures} Proposed head gestures and their corresponding actions\cite{yan2018headgesture}.}
    \Description[9 diagrams depicting the gestures proposed by \citeauthor{yan2018headgesture}]{9 diagrams depicitng the path of the nose taken to perform each gesture proposed by \citeauthor{yan2018headgesture}.} % TODO: Expand on this
\end{figure}
\\\\
An example of this can be seen in the work of \citeauthor{yan2018headgesture}\cite{yan2018headgesture} who propose nine head gestures (\autoref{fig:yan2018headgesture_proposed_gestures}), the majority of which are purely Semaphoric, however several (such as scrolling, dragging, and zooming) could also be seen as Manipulative through the mapped action physically moving the content on screen.
\citeauthor{yan2018headgesture} derived these gestures through a study wherein participants where asked to proposed a set of head gestures, without being given an associated action. These gestures were then collated manually into a set of 80 gestures, which were then effectively voted upon by the participants for their respective actions.
The gestures with the most votes for a given action were selected, with some minor adjustments to ensure there were no clashes between actions.

Another system that utilised multiple gestures was EyeMu\cite{kong2021eyemu}, which outlines several gestures that are performed by physically moving the smartphone, to improve user interaction when the user is forced to interact with phone single handed.
As with \citeauthor{yan2018headgesture}'s gestures, most are Semaphoric, but can be viewed as manipulative.
Some map to actual actions, e.g. flicking between items, others are less derivative and have less of a connection to the desired effect, e.g. moving phone closer/further from face to select an item / open a page.

% NOUSE
Two systems that were purely Deictic are Nouse\cite{gorodnichy2004nouse} and a system developed by \citeauthor{varona2008hands}\cite{varona2008hands}, both of which map the position of the user's nose, within images from the front-facing camera, to a location on the screen.
Neither system recognises sequences of motions of the head as gestures, other than recognising blinking and winking, which were recognised as an action to select what was under the cursor. 
You could also argue that these systems are also manipulative given they show a cursor and as such the head gestures are in an attempt to move the cursor to the relevant location.

% https://projet.liris.cnrs.fr/imagine/pub/proceedings/CVPR2012/data/papers/workshops/W01_05.pdf
One system which could be said to combine all three gesture styles would be the virtual 3D display proposed by \citeauthor{lopez2012head}\cite{lopez2012head}.
Their system treats the smartphone screen as a window into a 3D box, where the region of the interior rendered is controlled via the user adjusting the position of their head with respect to the screen.
\tempnote{Want to use figure from their paper, assume I can include screenshot as long as ref the page num and fig num?}
The tracking of relative positioning of the head to adjust the perspective of what is rendered can be seen as a form of both Deictic and Manipulative gesturing, as the user is looking at different regions within the interior of the virtual box by simply changing where they are looking, but with the intent to adjust the visible interior of the virtual box.
The system also provides Semaphoric gestures when interacting with specific programs; in one example they use a browser, with which they can look to the edge of the page to reveal the bookmarks bar.

% What conclusions can we make from this? 
% The types of gestures we expect (pointing, both manipulative and deictic, and semaphoric)

% Head bang and reach
% Initially used to 

% Phone gestures
% https://dl.acm.org/doi/pdf/10.1145/3462244.3479938

\subsection{Head Localisation}
% Image processing using front-facing camera
%   CNN (extract face position or pose/features)
%   HAAR Cascades (extract face position, or extended to features)
%   HSV Image Segmentation (extract face position, additional processing for pose etc)
%   Optical Flow (maybe extract face, or just use Optical flow to guess movement, might not need this one)
% IMU hardware (cap mounted IMU or headset)
%   Headset (not applicable)
%   Bespoke accessory with IMU, maybe use earbuds with IMU
% LIDAR/Depth Cameras (if time/space, if not, mention off-hand ignoring as not available on all devices, and not something you could buy after the fact, like headset or earbuds)
Before being able to distinguish between head and phone gestures, we first need to extract them. 
To start with we will be reviewing the methods in surrounding literature to extract relevant data required to track head gestures through the use of a smartphone.
\\\\
The naive approach often taken for finding a face, or more generally a person, within an image is to perform colour segmentation\cite{huang2004robust,bin2007rgb,chan2004face}, which involves taking an image and filtering the pixels based on a range of colour values that have been chosen as representing skin-tones.

While simple, and given favourable conditions, effective, this approach has several drawbacks:
\begin{enumerate}
    \item Detection of objects which have colours that have similar colour and chrominance levels, as noted by\citeauthor{bin2007rgb}\cite{bin2007rgb}.
    \item Determining the values with which to segment the image, i.e. what colours will we accept as skin-tone? During their system evaluation \citeauthor{chan2004face}\cite{chan2004face} used participants with similar skin-tones to improve the system's robustness.
    \item Dependence on environment lighting.\\
        All three of the papers above\cite{huang2004robust,bin2007rgb,chan2004face} do make use of the HSV/yCbCr colour spaces, which make them more robust to changes in lighting intensity, however these systems can still be susceptible to changes in lighting temperature, colour, or shadows.
\end{enumerate}\nl
% \subsubsection{HAAR Cascades / The Viola Jones Algorithm}\nl
A less naive approach is the Viola-Jones algorithm proposed in 2004\cite{viola2004robust}, used in digital cameras, smartphone camera apps, and several head gesture systems\cite{kim2017real, neto2012real, francone2011using}.
Rather than looking for skin-tone to find faces, it uses the difference of intensity between regions of pixels, and checks if they match a set of templates, Haar-features. These features compare the relative intensity of 2, 3, or 4 neighbouring regions, e.g. is the centre of a region brighter than the regions to the left and right. 
The algorithm proposed by \citeauthor{viola2004robust} uses a degrading cascading classifier\footnote{Where a traditional cascading classifier will have possibly have 2 branches at each node, a degrading one will always exit, returning nothing, on one of the branches of each node.} to apply these Haar-features on an integral image\footnote{A representation of the input image that permits an efficient means to calculate the sum of a rectangular region of the image with just the for corner points.} and will return a bounding box for each face found.

\citeauthor{kim2017real} build upon the Viola-Jones algorithm, still utilising Haar features but building their classifier to return the locations of four facial features: left eye, right eye, nose-tip, and mouth, in place of bounding boxes\cite{kim2017real}.

A more typical approach however is to use the Viola-Jones algorithm to retrieve the bounding box of faces within the image, and then perform further processing to extract facial features\cite{neto2012real, francone2011using, kim2017real}.
% \cite{neto2012real} Also utilise HAAR-like features in a classifier, however they simply use this to identify the region of the image that contains a face, and then utilise a histogram of pixel intensity (amount of variation in a given slice of the image) to identify the eyes.
% This is done once during calibration to then extract the eyes (left eye 30\% along the line, right is 70\%) and nose (if the eyes are $d$ far apart, the nose is $d*0.45$ below the midpoint between the eyes), assuming head is upright and user is facing camera.
One downside with the algorithm that \citeauthor{viola2004robust} note is that it cannot reliably detect faces that are rotated ±15\textdegree, while the person is still facing the camera, or ±45\textdegree, where the person is facing off to the side of the camera.
% An aside on use to extract location of face, but no features
% Application of filters representing features found on common faces.
% Applied in a cascade, e.g. only check for next set of features if found large one.
% Used to reduce search space / and limit data that needs processing.
\\\\
Another solution present in the literature, that is invariant to face pose, is the use of depth cameras. 
In particular we found the use of Apple's ARKit framework used alongside the front-facing depth camera (on supported iOS devices)\cite{voelker2020headreach,hueber2020headbang,deepateep2020facial}.
ARKit provides a reliable 3D representation/positioning of the face and its features  which could be used for tracking the head's movement.
The only downside of this approach seems to be the requirement of the hardware and OS in order to use the ARKit framework.
\\\\
The final solution we reviewed was the use of Convolutional Neural Networks (CNNs).
YuNet\cite{yu2022yunet} for example outputs the bounding box along with the positions of the eyes, nose tip, and the corners of the mouth. YuNet is in fact the replacement suggested by OpenCV for detecting faces, which previously used and recommended the Viola-Jones algorithm.

\citeauthor{yan2021fast} propose a CNN which instead predicts the roll, pitch, and yaw of a face provided within an input image\cite{yan2021fast}. With their CNN they were able to observe reduced error in their predictions compared to existing tools.

The benefit of a CNN is that they should be invariant of head rotation, given the training data includes samples of heads at different rotations.
A potential downside is the need for sufficient processing power. However there do now exist mobile variants of popular Neural Network frameworks, such as TensorFlow, with TFLite, and PyTorch, with PyTorch Mobile, which make running CNNs feasible on mobile devices.

% Android MLkit for face detection?

% Some use HAAR cascades to narrow down region
% Some alo output similar to HAAR, e.g. bounding box

\subsection{Phone Localisation}
% We now have several methods we can refer to in order to determine the head gesture being performed by a user, however we now need to explore the final area in the related works: recognising phone gestures. 
During our review of related works we came across several means of localising a smartphone / tracking a smartphone's movement, each with varying degrees of feasibility.

The least reasonable methods do not bare reviewing due to unrealistic expectations of the population of smartphone users, such as the need for a Motion Capture (MoCap) system\cite{buschel2017investigating} or the use of a Head Mounted Display with a mounted tracking marker\cite{mohr2019trackcap}.
These may be suitable in specific environments, but are not reasonable in meeting our goal.
\\\\
A more reasonable set of methods involve localising the smartphone's position relative to its environment.
\tempnote{Picture of features used for camera tracking}\\
One method is 'camera tracking', wherein the movement of the camera is estimated through analysis of an image stream from the rear-facing camera. This is a technique common-place in VFX to recreate the path taken by a camera in 3D\cite{barber2016camera}, but has also been extended to use in Augmented Reality applications\cite{jiang2000camera}.
Unfortunately this isn't reasonable to use on current modern mobile phones as they don't all support the ability to capture images from multiple cameras (some via software, others due to hardware limitations). As such we will not be able to utilise the rear camera as the front-camera will be required to track the user's face in our proposed system.

Another solution is the use of either Depth-Cameras or LiDaR and tracking the smartphone's movement through the observed 3D space.
Unfortunately these require special hardware that isn't available on most smartphones; most depth cameras that exist on modern smartphones are front-facing and the only current mainstream phones to provide a LiDaR on the rear of the phone are the iPhone 12 and 13 Pro series.
\\\\
The only method we found to be reasonable and feasible was to record the linear and angular acceleration of a smartphone's Inertial Measurement Unit (IMU)\cite{mantyla2000hand, kratz2013combining, neelasagar2015real, garcia2014contextualized}. 
An IMU provides the acceleration experienced in the 6 Degrees of Freedom (DoF)\footnote{3 Linear Axis: X, Y, Z, 3 Angular Axis: Yaw, Pitch, Roll (though this can also be expressed as a Quaternion to avoid gimble lock\tempnote{ref here, or just drop Quaternion note?})} the smartphone can be manoeuvred through.
A common issue however with processing IMU output is noise, as noted by \citeauthor{neelasagar2015real}. To address potential noise they utilised low and high pass filters on the acceleration data. 

\subsection{Gesture Recognition}
% Head Gesture Recognition
% How did reviewed systems do this?
% Given data, how determine gesture performed, e.g. Dietic pointing (use raw data, e.g. calculate screen region/pointer position from data) vs Semaphoric/Manipulative classifying (given data, determine gesture performed)
%   - Derived
%   - Regression NN?
%   - RNN/LSTM (papers not specific to mobile phones)
%       - given a sequence, what class does it belong to?
%   - Markov Chain
%       - Given current state and new input, what is the new state?
Knowing how we can obtain facial features and the 'pose' of the user's head through a front-facing camera, and the localisation of the smartphone itself, the next step is to be able to recognise gestures performed by the user with either their head or the smartphone.

% \subsubsection{Relative Positioning}\nl
% NN vs Function, how best to phrase?
% Used for Pointing (Dietic)
% Great if data in is accurate (IMU may not be ideal without filtering)
One solution employed by papers proposing systems that tracked Deictic pointing gestures (and possibly Manipulative pointing gestures), was to simply use the raw data, or a function of the data, to map detected facial features to a location within the UI.

A common approach was to take the position of the nose and map it to a point on the screen. This could either be used to manipulate a cursor\cite{gorodnichy2004nouse, varona2008hands, onuki2016combined}, allowing the user to move their head to highlight specific places on the screen, or to highlight the region of the phone the user is looking\cite{hueber2020headbang,voelker2020headreach,roig2015face}.
% \subsubsection{Recurrent Neural Network (RNN)}\nl
% Write out the 
% Given a sequence, classify the type of motion
% not ideal if don't have known fixed number of frames, unless staggered
\\\\
For semaphoric gestures you need to be able to be able to identify the gesture within a sequence of input.
An RNN is a Neural Network that takes a sequence of elements and has an internal state that is updated by some function of the current element being processed and the current state.
\citeauthor{sharma2018recognizing} proposed the use of an RNN in order to recognise head gestures, wherein the RNN input was a sequence of facial landmarks extracted from a sequence of images\cite{sharma2018recognizing}. 
An advantage of using an RNN is that you don't strictly need to know exactly when the in the sequence the gesture was performed, just that it is present within the sequence.
A downside however is that internal state isn't maintained between predictions, as such you \textit{must} provide a sequence and the input sequence \textit{must} always be of a fixed length\footnote{\tempnote{metion that best to our knowledge this isn't supported}}. Input must there for be broken-up to fixed lengths, either requiring padding prior to/after the gesture recorded (if you do not have enough elements for the required sequence). To break-up the input you need to either run the model each time-step, providing a rolling window representing the last $x$ frames of state, or to have another means to segment your recorded input to then pass into the RNN.
\\\\
% \subsubsection{Hidden Markov Model (HMM)}\nl
% Given current state and new input, determine the new state
% Great if continuous input
% Another way that a Semaphoric gesture can be described is via a set of possible states, e.g. head poses or movement, and a set of rules which describe how these states can change.
Another method for predicting Semaphoric gestures from a sequence, is to use Hidden Markov Models (HMMs)\cite{elmezain2008hidden, terven2014robust}.
A HMM describes the possible hidden states a system can be in, the probabilities/rules for transitioning from one state to another, the sates that can be observed, and the probability that a given observation arises from each hidden state.
For example, the HMM employed by \citeauthor{elmezain2008hidden}\cite{elmezain2008hidden} has the gestures as its hidden states, in this case arabic numbers, with the possible observations being a quantised direction\footnote{To reduce the possible observation space, the angles from 0\textdegree to 360\textdegree are bucketed into a range of 0 to 18} that the user's hand travelled, captured via a camera. The probabilities of the sequence of observations would be observed for a given number drawn by the user can then be trained.


% \subsubsection{Dynamic Time Warping (DTW)}
% regarding phone gestures...Others however utilised a technique called Dynamic Time Warping (DTW), sometimes used in conjunction with a HMM.
% \tempnote{refs}\nl

% \textbf{OLD}
% \\
% ----------------------------------------------------
% \\
% %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% % Review of head gestures / study?
% % Gestures as form of control
% One of the simpler ways to track the movement of a user's head is with an Inertial Measurement Unit
%  (IMU), as this is a physical device that can be used to measure rotational and linear acceleration\footnote{Linear acceleration is typically less accurately tracked compared to angular acceleration}.



% The gestures they propose were derived from a study wherein they asked participants to suggest head movements that they believed corresponded to the action taken.
% These were then collated by manually into 80 gestures, which were then effectively voted upon by the participants for their respective actions.
% The gestures with the most votes for a given action were selected, with some minor adjustments to ensure there were no clashes between actions.

% To extract the gestures from the Hololens, the IMU output was segmented via detection of acceleration (20 degrees per second) and deceleration (4 degrees per second), not exceeding 2 seconds.

% Feature extraction is performed with Dynamic Time Warping (DTW)\cite{berndt1994using}, followed by a Support Vector Machine (SVM) classifier to classify the observed gesture into one of 9 the categories, or unintentional movement. With just the DTW they were able to achieve 90\% accuracy, but with the SVM they were able to boost this to 97\%.

% To evaluate the head gestures, they compared them with existing hand gestures. They found that head gestures caused more fatigue and generally felt less natural, while being equivalent or better with regards to learnability.

% % Gestures as interface interactions
% % Closer to mobile, as have just reliance on cameras
% Using a mobile device we won't have access to an IMU on the user's head, however we will be able to try and utilise the same set of gestures, and to use a similar approach to track the phone's movement, which could allow us to try and differentiate between the phone or head being moved.
% \\\\
% An alternative approach is to try and extract the face using an RGB camera.

% One way to do this was developed by \citeauthor{gorodnichy2002importance}, which 'finds' the nose under the assumption that it should have the greatest intensity gradient since it should always be closest to the camera, and given it is convex in nature it should be the 'brightest' feature\cite{gorodnichy2002importance}.

% The tracked point isn't a specific point on the nose (e.g. the tip), but a point that can move across the surface of the nose, based on what is closest to the camera.

% They go on to extend this work with the usage of the user's nose to control a pointer on their screen\cite{gorodnichy2004nouse}. 
% They extract just the nose since it meets their two requirements for a trackable feature: \begin{enumerate}
%     \item It is always visible, presuming the user is facing within 180 degrees towards the camera.
%     \item Only one feature should be used to define the cursor, to reduce/eliminate potential jitter.
% \end{enumerate}

% To click with the 'Nouse' the user blinks twice within short succession. Blinking is determined by reviewing the change in the sequence of 3 frames.

% To ensure the system is realtime they use a reduced resolution, and could find that a resolution of 160x120 was robust enough to accurately track nose, and map the cursor to an accurate location on the screen.

% They make claims about accuracy and enjoyment, but relevant data not provided. They only seemed to make statements suggesting Nouse was as good as, if not better than, typical mouse control.
% They claim mouse usage caused wrist ache, but movement of entire head doesn't present neck ache, which was reported in the IMU head tracking describe4d above\cite{yan2018headgesture}.

% Another nose controlled cursor is presented by \citeauthor{varona2008hands}, however they use Haar cascades to extract the region containing the face, within which they use a similar technique as above\cite{gorodnichy2002importance} to extract points for the corners of the nose, or the nostrils\cite{varona2008hands}.

% To detect eyes, the system determines the user's skin colour by sampling the pixels within the detected face region. They then presume the eyes will be a different colour, and as such filter based on the extracted skin-tone. They then select the features closest to the nose, that are symmetrical.

% A UI is provided with possible actions as buttons. The user moves the cursor to the action they wish to perform, then wink (with either eye) to select it. When they then fixate on part of the screen (move the cursor to a point and keep it stationary), the action will be performed.

% Only evaluated for click recognition and accuracy of where the click was performed within a grid of points. However >80\% accuracy even for users with no training time, just instructions.

% Moving closer to a tool for mobile devices we have the work of \citeauthor{roig2015face}, who use the front faced camera to scroll, using the head angle w/r/t the device as direction of scrolling.\cite{roig2015face}

% They extend upon the work of \citeauthor{varona2008hands}\cite{varona2008hands} the nose of the user and to correlate it's motion to a virtual cursor on the screen.

% Selection/tapping is performed via tapping anywhere on the screen, however the tap will actually occur under the virtual cursor.

% They evaluated the system by asking users to select elements of varying sizes, phone held in different orientations (portrait vs landscape), and with varying gain applied to the velocity of the cursor in response to head movement.

% Elements below 88x88pt\footnote{Apple Point, effectively 2 pixels on a retina display, so 88pt == 176px} were found to be less successfully selected, this is primarily due to the low resolution used for the gesture tracking being unable to be mapped to a finer resolution on the device screen. Potentially increasing the resolution used for the tracking could permit finer accuracy.

% They do not distinguish between the user moving the phone, or the user moving their head. It could be seen as a feature, either move the phone or head to scroll, but this would be intersting to try and distinguish, to potentially support additional actions/gestures.

% The above systems describe the ability to identify where on a screen the user is looking, or at the very least intends to perform some action, through providing them with a virtual cursor they can manipulate via moving there head, either through rotation or physically moving the head.
% We can look to extend upon this to understand where the user's attention may be focused.
% \\\\
% Some smartphones now also include front-facing depth cameras, a technique for tracking a user's head, and detecting head/facial features is provided by \citeauthor{deepateep2020facial}, wherein they utilised the ARKit Framework for iOS\cite{deepateep2020facial}.

% Objective similar to works described above to control a virtual cursor, however instead of specifically using the nose, they are using the perceived pose of the user's entire head.

% Cursor motion is tracked based on head pitch and yaw in the Y and X directions respectively. Requires user to directly face the camera for zero movement of the cursor.

% Additionally they combine this control with facial gestures, which perform specific actions, or permit the beginning of specific actions, such as zoom, drag, and tapping.
% These utilised poses obtained from the eyebrows and mouth. Timings specific to each action, some gestures overloaded based on timing.


% A depth camera affords greater accuracy for tracking (particularly for understanding the distance from the screen), and in case of iPhone there is consistency with specific hardware. 
% However for the general smartphone population, specifically android, depth cameras aren't standard, and when present can have different hardware. 
% For our project we will presume 3D depth cameras aren't available.
% % Spatial 
% % \cite{voelker2020headreach} developed a tool to combine head orientation to move a cursor into a particular section of the screen, from which they can then use relative motion of the thumb to adjust the cursor to the element of interest.\\
% % Utilise the ARKit Framework for iOS.\\
% % Utilise 3D-Touch (on Apple device used, application of force can be used to differentiate between a light or heavy touch) to activate the head-tracking mode.
% % They presume that when mode engaged that user is facing phone to the extent they wish, and use this to calibrate the 'origin' position of the user's head, rather than requiring the user to face the camera.\\
% % Tested 3 different interaction techniques.
% % Head Cursor Only - once mode engaged, user moves head to where they want to select, and upon release current target is selected.
% % Head Cursor and Touch - move cursor in same way, but applying more force then allows them to move their thumb to make adjustment to the cursor position.
% % Head Area and Touch - rather than having a cursor controlled by the head, the head is used to select a quadrant of the display, then as with above technique the user can then move the cursor with their thumb to select a specific target.\\
% % They compared this with direct touch and an existing thumb-only technique called BezelCursor \cite{li2016bezelcursor}, which permits a user to move a virtual cursor along the entire screen with just movements of their thumb.\\
% % They found that while standing, direct touch was fastest, followed by the head tracking methods, with the BezelCursor being slowest. However accuracy was effectively reversed in order.\\
% % While walking this was similar, however the Head Cursor Only approach was slowest overall and less accurate than direct touch.

% % % Semantic & Spatial
% % This was then extended upon \citep{hueber2020headbang} to use the same tracking technology to instead perform gestures.\\
% % activated by lightly touching the element, followed by performing the gesture. The detected action is shown as a pop-up and can be selected by releasing the touch, or cancelled by moving the touch out of the element and then releasing.\\
% % Gestures were made simple based on the direction the user looked away, with actions effectively being placed within a disk, with each action getting an equally sized segment.
% % Performing a gesture requires a user to remember the direction associated with the action they wish to perform.\\
% % Then then compared to other techniques that work off similar principle (using a segmented wheel to represent different actions), some requiring the device itself be moved, or the thumb, and one other technique that instead uses a vertical list in-place of a wheel.\\
% % HeadBang was observed with the greatest accuracy of the wheel segmented approaches for all menu sizes, however was out-performed by the vertical list for all but the smallest menu size.

% % Fair to say, or need data proof?

% % Basically what we were thinking of doing, but without an adaptive interface...
% % Spatial 
% % \cite{hansen2006use} Tool to use front-facing camera to track phone movement relative to user's face.
% % Face extracted through histogram (user places face in centre of frame and clicks button to calibrate their skin-tone), it then determines the range of hue for the face based on a region of pixels in the centre of the frame, and uses this to bin the pixels into face and background based on the hue.\\
% % The size, centre and rotation of the extracted region is then computed, this is remembered between frames to reduce the impact of changes in background, as 
% % They then use this input to evaluate 3 applications (image viewer, Bluetooth connections, pong).\\
% % They highlight that there is an issue with moving device vs tilting, but offer no solution, camera FOV can reduce action-space, and that accessibility issues with moving the phone, making it harder to read.

% % \subsubsection{Types of Gestures} % Before or after Mobile devices?

% % \cite{aigner2012understanding} Types of gesture

% % From the head gesture systems reviewed above, we can classify them into 2 groups of gesture types.
% % Summarising from \pcite{clarke2020dynamic} thesis, gestures can be defined as being "semantically mapped to a set of corresponding actions", or Spatially mapped, although the gestures themselves may occur spatially in motor space. Manipulative and pointing gestures involve a spatial mapping, and usually a logical abstraction of the user’s attention in the form of a cursor"
% % \begin{description}
% %     \item[Semantic] Wherein the gesture is mapped to a specific action within the action space. Such as a static hand-pose (e.g. thumbs-up), or a specific sequence of poses (e.g. 
% %     \item[Spatial]  The gesture is mapped to some value-space, the value corresponds directly to the gesture pose. Such as the movement of a pointer on a screen, or the position of a slider or dial.
% % \end{description}


% % \subsection{Adaptive Interfaces}
% % % Start with general examples, then refine towards mobile devices, then if possible, refine to head controlled

% % % \cite{wesson2010can} Describes the types of adaptive UI (e.g. what contexts the UI is aware of) 

% % \subsubsection{3D Interfaces} % Projected to 2D display
% % % An alternative to tabs/pages in applications, have application interface in 3D, and only expose based on perspective
% % % Perspective based on phone vs user
% % % Examples of AR/VR, display adapts

% % % \cite{buschel2017investigating} Using IR camera array and trackable tablet (attachments to track via IR cameras), track the 3d position and rotation of device within space.\\
% % % Position and rotation then used to adjust the rendered view of the 3D graphs (though could be extended to more 3D models generally). Allows user to view different perspectives and focus on specific regions though physical movement of the device, rather than with just touch input.\\
% % % Compared 3 different tasks (finding specific information, comparing size of objects, and structural understanding (selecting a similar plot))\\
% % % Fatigue was highest for the device movement, compared to touch, however outperformed touch significantly with regards to perceived control of the viewport, and perceived success.\\
% % % They couldn't find any significant difference in completion times or success rate for the comparative and structural understanding tasks, but could observe that the navigation task was generally completed faster when utilising the touch only interface. this could be due to the size of the virtual environment, requiring users to move to find items, while touch interaction permitted zooming out and then back in.
% % A 3D interface presumes that there is a virtual screen through which a portion is visible from the mobile device.

% % \citeauthor{francone2011using} approaches this by developing a system which adjusts 3D content on the screen based on the user's perspective/orientation relative to the phone screen/front facing camera\cite{francone2011using}.

% % Their system utilises Haar cascades to extract the user's face. The X and Y positions are tracked via the centre of the observed bounding box. Here they intentionally accept both movement of  either the head or device.
% % Depth is estimated via the size of the region, however this fails if the user's face starts to go out-of-frame of the camera, as the a portion of the face may still be recognise, resulting in a thin bounding box.

% % For their interface they tested:
% % \begin{itemize}
% %     \item Displaying 3D interface elements, e.g. adding depth to the interface such that adjusting perspective would permit viewing the sides of elements within the UI.
% %     \item A workspace that was too large to fit onto the screen, and to reveal other elements of the interface you could adjust the perspective, rather than say changing the page.
% % \end{itemize}
% % For their evaluation they did not compare with existing techniques, such as touch, but rather asked participants to directly evaluate the usability in isolation.

% % We can learn from \citeauthor{francone2011using} regarding the depth axis. To alleviate the issue wherein the face is partially out-of-frame we could adjust the face detection to require the whole face, or to understand how much of a face has been detected.
% % We can also follow a similar evaluation procedure, however it would be beneficial to also compare to an existing interface.
% % \\\\
% % Another approach is developed by \citeauthor{miyazaki2021ar}, however they approach this from the perspective of the phone moving to expose a virtual display\cite{miyazaki2021ar}.

% % They developed a virtual display (a map within which several 'pins' are present), which could either be viewed as static and hovering in-front of the user, or placed upon a 2D plane in their environment.
% % To move around the virtual display, they simply need to move the device.
% % To do this they utilised Google's Ar toolkit: Tango. The rear camera of the device would be used by Tango to track the device, and to track the position of the virtual display, either with respect to a 2D plane, or the user.

% % They evaluated these two displays against a touch-only display, which could only be manipulated by touch.
% % They found that the touch only implementation was the slowest for finding various the pins.
% % Both virtual display interfaces were found to require less mental load (like recalling the direction of various points on the map from the current position), and intuitive to use when compared with touch only, though the non-AR instance was found to be more taxing to use.

% % For our project we can look into also utilising some AR features to help track the phone, and differentiate the movement of the head vs movement of the phone.
% % However we would need to investigate power usage and processing limits if we are trying to perform AR tracking and head tracking, using both front and rear cameras.

% % \subsubsection{Context Aware UI}
% % % One way to improve usability is to have the UI elements adapt to the user context
% % % based on:
% % %   prior actions
% % %   current area of focus (enlarging / moving elements to make easier for user to interact)
% % %   user goals

% % % AR
% % Where the 3D UI allows a user to move around and adjust the perspective of the UI elements, an alternative approach would be to instead change which UI elements are on screen in reaction to the user's attention or perceived intentions.

% % An example of this by \citeauthor{pfeuffer2021artention} is a UI for an AR head mounted display which adjusts the displed content and presented information based on user's gaze\cite{pfeuffer2021artention}.

% % This is primarily controlled via user gaze, specifically dwell-time, however is also informed by the task context.
% % For example in their conversational UI, information about a person is presented around them, in the background, however if the user were to dwell upon specific information, it will bring it to the foreground, and display more (if applicable), until they revert their gaze back upon the person.
% % In the tree of life example (a network graph for a subset of the tree of life is rendered), there is no 'background', instead gaze is used to adjust the level of information for specific nodes.
% % This is also the case for the shopping interface, where gaze permits information to display for specific items, and the ability to interact with said items.

% % They evaluated their interfaces with different amounts dwell-timerequired to adapt/accept user input, to compare the accuracy and error rate.
% % With shorter dwell times task completion was faster, as expected, but the error rate much higher. They found that a 3 sec dwell-time was least error prone (in a range of 1-4s).
% % However the majority of participants found the dwell-time was too slow for anything beyond 2s.

% % We can't use this directly, as we won't be using gaze, but we could adapt to work with a cursor controlled by the user's head.
% % \\\\
% % % Gestures more generally ?
% % % \cite{clarke2020reactive} adapts presented video based on gesture recognition

% % % Specific to mobile device
% % % The work by \citeauthor{yigitbas2019context} \cite{{yigitbas2019context, yigitbas2019component}} evaluate 2 similar applications that react to various user environment states, and previously learned information about the user.
% % % Second one can be dropped?
% % % Specific to head reaction
% % Another interpretation of this is the interface developed by \citeauthor{lopez2012head}, which is a virtual display that is in the shape of a concave box, from which the visible part of the box is based on the user's perspective.\cite{lopez2012head}. 
% % This is similar to the work of \citeauthor{francone2011using}, however specifically for extending workspaces, and for supporting adaptive UI elements which react to the user's head position.

% % One interface is described as being a concave box, from which interface elements are placed. When the user places attention to a particular element, it is brought into the foreground for use, and can then be dismissed for them to select another element.
% % Another interface they describe permits head gestures to be recognised which map to revealing additional UI elements based on user intent. Their example involves a web-page, wherein turning/moving in from the right would reveal a prompt to add the page to the bookmarks, and continuing the motion would open the bookmark dialogue.
% % % Also looks into whether the head or phone moving

% % The head is extracted via an Adaboost classifier which determines the weighting of a 20x20px neighbourhood of pixels as containing facial features, in similar manner to Haar cascade.
% % head distance is then estimated via change in scale of the extracted region, so no definitive size, but permits a relative scale/distance from original size.

% % Due to limited Field of View (FOV) of the camera, they added a wide-angle lens to increase the FOV to 160 degrees.

% % Unfortunately they don't actually implement an instance of any of their proposed interfaces, which is something we can look to achieve and evaluate.


% % % Summary?

% % %% QUOTES & NOTES %%


% % % Two points of interest:\\
% % % - Input modalities: to address limited reach of thumb\\
% % % - Adaptive Interfaces: To adjust visible elements / positioning of elements based on context\\

% % % % Would start with thumb gestures, but is this needed?
% % % % Can look to combine head movement with thumb?
% % % % Start with more cumbersome / excessive gestural techniques, refine towards thumb + head/gaze

% % % % Discuss determining if head vs phone is moving

% % % \subsection{Input Modalities}
% % % \subsubsection{Phone Gestures}
% % % % @inproceedings{ti2013tiltzoom,
% % % %   title={TiltZoom: tilt-based zooming control for easy one-handed mobile interactions},
% % % %   author={Ti, Jimmy and Tjondronegoro, Dian},
% % % %   booktitle={Australian Computer-Human Interaction Conference (24th)},
% % % %   pages={1--1},
% % % %   year={2013}
% % % % }
% % % Though limited in functionality, \pcite{ti2013tiltzoom} TiltZoom tool permits a user to adjust the level of zoom of a map via tilting the phone away (zoom-out) and towards (zoom-in) the user.\\
% % % This is due to typical zooming gestures requiring 2 digit input (e.g. pinching the display), which often requires two-handed interaction with the phone.

% % % Though apps such as google images and maps now support a double-tap and drag gesture to perform zoom operations, the usage of physical device rotation did show that it could be a usable and accessible user input.\\
% % % However one downside that was observed was a tiring of the wrist.

% % % % @inproceedings{chen2012extending,
% % % %   title={Extending a mobile device's interaction space through body-centric interaction},
% % % %   author={Chen, Xiang'Anthony' and Marquardt, Nicolai and Tang, Anthony and Boring, Sebastian and Greenberg, Saul},
% % % %   booktitle={Proceedings of the 14th international conference on Human-computer interaction with mobile devices and services},
% % % %   pages={151--160},
% % % %   year={2012}
% % % % }
% % % \cite{chen2012extending} took a different approach to phone-based gestures. Rather than developing a technique to convert the phone positioning/movement into an analog input, they instead looked to develop a system that used fixed / incremental input, that treated the phone position relative to the body as an action\\
% % % More akin to pressing max / min volume vs slowly incrementing the volume.\\
% % % Their system permits users to 'place' objects (such as urls, images, calender appointments) with respect to their body, which can then be retrieved at a later time.\\
% % % This could be interpreted as having a virtual space around the body, wherein the phone acts as a cursor to interact with elements within the space.

% % % \subsubsection{Head Gestures}
% % % Either tracking head / face to act as a pointer, or to be interpreted as a gesture.
% % % % Spacial and semantic input

% % % % @inproceedings{roig2015face,
% % % %   title={Face Me! Head-tracker interface evaluation on mobile devices},
% % % %   author={Roig-Maim{\'o}, Maria Francesca and Varona G{\'o}mez, Javier and Manresa-Yee, Cristina},
% % % %   booktitle={Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems},
% % % %   pages={1573--1578},
% % % %   year={2015}
% % % % }
% % % \cite{roig2015face} Using front faced camera to scroll, using the head angle w/r/t the device as direction of scrolling.\\


% % % % @article{onuki2016combined,
% % % %   title={Combined use of rear touch gestures and facial feature detection to achieve single-handed navigation of mobile devices},
% % % %   author={Onuki, Yoshikazu and Kumazawa, Itsuo},
% % % %   journal={IEEE Transactions on Human-Machine Systems},
% % % %   volume={46},
% % % %   number={5},
% % % %   pages={684--693},
% % % %   year={2016},
% % % %   publisher={IEEE}
% % % % }
% % % \cite{onuki2016combined} Using front facing camera to track user's head orientation (used to move a cursor), and the distance between the user's eyes to infer distance from the display, which in turn adjusts the coarseness of the cursor locations by effectively zooming in/out.

% % % % @inproceedings{voelker2020headreach,
% % % %   title={HeadReach: Using Head Tracking to Increase Reachability on Mobile Touch Devices},
% % % %   author={Voelker, Simon and Hueber, Sebastian and Corsten, Christian and Remy, Christian},
% % % %   booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
% % % %   pages={1--12},
% % % %   year={2020}
% % % % }
% % % \cite{voelker2020headreach} developed a tool to combine head orientation to move a cursor into a particular section of the screen, from which they can then use relative motion of the thumb to adjust the cursor to the element of interest.\\

% % % % @inproceedings{hueber2020headbang,
% % % %   title={Headbang: Using head gestures to trigger discrete actions on mobile devices},
% % % %   author={Hueber, Sebastian and Cherek, Christian and Wacker, Philipp and Borchers, Jan and Voelker, Simon},
% % % %   booktitle={22nd International Conference on Human-Computer Interaction with Mobile Devices and Services},
% % % %   pages={1--10},
% % % %   year={2020}
% % % % }
% % % This was then extended upon \citep{hueber2020headbang} to use the same tracking technology to instead perform gestures.\\
% % % Gestures were made simple based on the direction the user looked away, with actions effectively being placed within a disk, with each action getting an equally sized segment.\\
% % % Performing a gesture requires a user to remember the direction associated with the action they wish to perform.
% % % % Unclear if tested success using different number of segments

% % % Unsure if any of the above could determine if head or phone was being moved/rotated

% % % % @article{yan2018headgesture,
% % % %   title={Headgesture: hands-free input approach leveraging head movements for hmd devices},
% % % %   author={Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun},
% % % %   journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
% % % %   volume={2},
% % % %   number={4},
% % % %   pages={1--23},
% % % %   year={2018},
% % % %   publisher={ACM New York, NY, USA}
% % % % }
% % % \cite{yan2018headgesture} also developed a system for performing gestures with the user's head, however they utilised more complicated gestures. Made for hands-free, rather than extending touch input.\\
% % % They determined the gesture movements via a study, resulting in 9 gestures/actions.\\
% % % Each action was to be a substitute for an existing gesture that could be performed with touch, such as tapping, scrolling, and zooming.\\
% % % Tracking was performed with hololens (not from phone).\\
% % % Was evaluated against Air-Tap, hololens extracting hand gestures.


% % % % @inproceedings{hansen2006use,
% % % %   title={Use your head: exploring face tracking for mobile interaction},
% % % %   author={Hansen, Thomas Riisgaard and Eriksson, Eva and Lykke-Olesen, Andreas},
% % % %   booktitle={CHI'06 Extended Abstracts on Human Factors in Computing Systems},
% % % %   pages={845--850},
% % % %   year={2006}
% % % % }
% % % % Basically what we were thinking of doing, bu without an adaptive interface...
% % % \cite{hansen2006use} Tool to use front-facing camera to track phone movement relative to user's face.
% % % They then use this input to evaluate 3 applications (image viewer, bluetooth connections, pong).\\
% % % They highlight that there is an issue with moving device vs tilting, camera FOV can reduce action-space, and that accessibility issues with moving the phone, making it harder to read.

% % % \subsubsection{Gaze / Eye Tracking}

